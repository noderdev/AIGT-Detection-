AI Generated Text Detection
==
Most of the existing AIGDT work at document level. However, a key limitation of existing AIGT detection strategies is their focus on discriminating entire documents as AI generated. This approach overlooks the common user behavior of modifying partial documents with LLMs rather than whole documents. Our approach helps to detect if a sentence is being generated by an AI model or not and or also tell which model the text was generated from. We have provided a trained model here https://github.com/noderdev/SeqXGPT_AIGT/blob/main/SeqXGPT_AIGT/linear_en.pt. You can us the mode directly.

Data Generation
=
All the data files for corresponding models can found here :
```
https://drive.google.com/file/d/1t-QTqm2pT-jxhKR29g6LB_j354rChkFL/view?usp=sharing
```
We have to merge all these sentences to have entire data for our model to train. Merging can automatically done using the notebook mentioned below.

Feature Generation
=
Need to deploy api's corresponding to various models for inference of perplexities present in https://github.com/noderdev/SeqXGPT_AIGT/blob/main/SeqXGPT_AIGT/backend_model.py

commands for the api deployment:
```
python backend_api.py --port 6006 --timeout 30000 --debug --model=gpt2 --gpu=0
python backend_api.py --port 6007 --timeout 30000 --debug --model=gptneo --gpu=0
python backend_api.py --port 6008 --timeout 30000 --debug --model=gptj --gpu=0
python backend_api.py --port 6009 --timeout 30000 --debug --model=llama --gpu=0
```
Once we have deployed our model and all the models are exposed as an api, we can get the features and form new files by running the note book : https://github.com/mpremashish/SeqXGPT_AIGT/blob/main/SeqXGPT_AIGT/dataset/gen_feature.ipynb

We will get the following files as output:
```
1) input.jsonl
2) output_all.jsonl
3) output_ prompt_merged.json
4) output_binary_merged.json
```

Significant files for training are output_ prompt_merged.jsonl and output_binary_merged.jsonl.

```
1) output_ prompt_merged.jsonl for Mixed-Model Multiclass
2) output_binary_merged.jsonl for Mixed-Model Binary
```
Move them accordingly for training to https://github.com/noderdev/SeqXGPT_AIGT/tree/main/SeqXGPT_AIGT/SeqXGPT (Warning: dont move both of them together, first one when training for multiclass and 2 one for binary classificaiton)

Training Generation
==
For training for multiclass, run (run in the model in following https://github.com/noderdev/SeqXGPT_AIGT/tree/main/SeqXGPT_AIGT):
```
python ./SeqXGPT/train.py --gpu=0 --split_dataset --data_path=./SeqXGPT --train_path=./SeqXGPT/train.jsonl --test_path=./SeqXGPT/test.json
```

For training for binary, run (run in the model in following https://github.com/mpremashish/SeqXGPT_AIGT/tree/main/SeqXGPT_AIGT):
```
python ./SeqXGPT/train.py --gpu=0 --split_dataset --data_path=./SeqXGPT --train_path=./SeqXGPT/train.jsonl --test_path=./SeqXGPT/test.jsonl --mixed_model_binary
```

Testing
==
For running on test mode with saved model:
```
python ./SeqXGPT/train.py --gpu=0 --do_test --test_path=./SeqXGPT/test.jsonl --train_path=./SeqXGPT/train.json
```
Error Analysis
==
After Test you will find a error analysis file named errors_analysis for 5 text that were not properly classified.

Multilingual
==
We made the model lingual by making tokenization changes and using different perplexity generation models. We made model multilingual especially to understand french and chinese. We need to train on the top of already trained model, following command we help up us achieve that. All the multilingual dataset can be found in drive provided above

```
python ./SeqXGPT/train.py --gpu=0 --split_dataset --data_path=./SeqXGPT --train_path=./SeqXGPT/train.jsonl --test_path=./SeqXGPT/test.jsonl --used_saved_model
```

Evaluation multilingual
==
We did the evaluation on multilingual dataset using binary classification of AI and Human using below command
```
python ./SeqXGPT/train.py --gpu=0 --split_dataset --data_path=./SeqXGPT --train_path=./SeqXGPT/train.jsonl --test_path=./SeqXGPT/test.jsonl --mixed_model_binary
```
