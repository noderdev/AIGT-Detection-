{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e134d7b-d6f0-497c-9ab1-d9a0ad3cf8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import httpx\n",
    "import msgpack\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e291521-bc02-47cf-bd71-f93bc57d41d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: jsonlines in /home/pmukher/.local/lib/python3.9/site-packages (4.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from jsonlines) (21.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49188de8-c6bd-4141-961a-614edd0a9f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_2_api = 'http://0.0.0.0:6006/inference'\n",
    "gpt2_french = 'http://0.0.0.0:6007/inference'\n",
    "gptj_french = 'http://0.0.0.0:6008/inference'\n",
    "llama_api_french = 'http://0.0.0.0:6009/inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71ae99f7-cb71-4017-9fad-b428b2467139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "def merge_jsonl(input_files, output_file):\n",
    "    combined_data = []\n",
    "    for file in input_files:\n",
    "        with open(file, 'r') as f:\n",
    "            for line in f:\n",
    "                json_data = json.loads(line.strip())\n",
    "                combined_data.append(json_data)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        for item in combined_data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "# Example usage\n",
    "input_files = glob.glob('./SeqXGPT-Bench/*.jsonl')  # List of JSONL files in current directory\n",
    "output_file = 'input.jsonl'\n",
    "\n",
    "merge_jsonl(input_files, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eadf911-f85a-48d3-b027-a465ec41bf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting googletrans==4.0.0-rc1\n",
      "  Using cached googletrans-4.0.0rc1-py3-none-any.whl\n",
      "Requirement already satisfied: httpx==0.13.3 in /home/pmukher/.local/lib/python3.9/site-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
      "Requirement already satisfied: certifi in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2021.10.8)\n",
      "Requirement already satisfied: hstspreload in /home/pmukher/.local/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.4.1)\n",
      "Requirement already satisfied: sniffio in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.2.0)\n",
      "Requirement already satisfied: chardet==3.* in /home/pmukher/.local/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in /home/pmukher/.local/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in /home/pmukher/.local/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in /home/pmukher/.local/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in /home/pmukher/.local/lib/python3.9/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in /home/pmukher/.local/lib/python3.9/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in /home/pmukher/.local/lib/python3.9/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in /home/pmukher/.local/lib/python3.9/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
      "Installing collected packages: googletrans\n",
      "  Attempting uninstall: googletrans\n",
      "    Found existing installation: googletrans 3.1.0a0\n",
      "    Uninstalling googletrans-3.1.0a0:\n",
      "      Successfully uninstalled googletrans-3.1.0a0\n",
      "Successfully installed googletrans-4.0.0rc1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting googletrans==3.1.0a0\n",
      "  Using cached googletrans-3.1.0a0-py3-none-any.whl\n",
      "Requirement already satisfied: httpx==0.13.3 in /home/pmukher/.local/lib/python3.9/site-packages (from googletrans==3.1.0a0) (0.13.3)\n",
      "Requirement already satisfied: certifi in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.10.8)\n",
      "Requirement already satisfied: hstspreload in /home/pmukher/.local/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2024.4.1)\n",
      "Requirement already satisfied: sniffio in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.2.0)\n",
      "Requirement already satisfied: chardet==3.* in /home/pmukher/.local/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in /home/pmukher/.local/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in /home/pmukher/.local/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in /home/pmukher/.local/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in /home/pmukher/.local/lib/python3.9/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in /home/pmukher/.local/lib/python3.9/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in /home/pmukher/.local/lib/python3.9/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in /home/pmukher/.local/lib/python3.9/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n",
      "Installing collected packages: googletrans\n",
      "  Attempting uninstall: googletrans\n",
      "    Found existing installation: googletrans 4.0.0rc1\n",
      "    Uninstalling googletrans-4.0.0rc1:\n",
      "      Successfully uninstalled googletrans-4.0.0rc1\n",
      "Successfully installed googletrans-3.1.0a0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: google_trans_new in /home/pmukher/.local/lib/python3.9/site-packages (1.1.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install googletrans==4.0.0-rc1\n",
    "!pip install googletrans==3.1.0a0\n",
    "!pip install google_trans_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3e5551-cccc-4524-ab3b-fbf3a3295bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googletrans\n",
    "from googletrans import Translator\n",
    "import jsonlines\n",
    "\n",
    "# see available languages with the below\n",
    "def translate(input_file,output_file,lan):\n",
    "    # print(googletrans.LANGUAGES)\n",
    "\n",
    "    # Init\n",
    "    translator = Translator()\n",
    "    \n",
    "    input_json = []\n",
    "    input_json_text = []\n",
    "    \n",
    "    with open(input_file, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            input_json.append(data)\n",
    "            input_json_text.append(data['text'])\n",
    "    \n",
    "    batches = []\n",
    "    batch_size = 100\n",
    "    for i in range(0,len(input_json_text), batch_size):\n",
    "        batch = input_json_text[i:i + batch_size]\n",
    "        batches.append(batch)     \n",
    "    \n",
    "    input_json_text = []\n",
    "    \n",
    "    for index,batch in enumerate(batches):\n",
    "        outputs = translator.translate(batch, src='en', dest=lan)\n",
    "        input_json_text.extend(outputs)\n",
    "        print('batch '+str(index)+' done, batches left'+str(len(batches)-1-index))\n",
    "            \n",
    "    \n",
    "    for index,inputs in enumerate(input_json):\n",
    "        inputs['text'] = input_json_text[index].text\n",
    "            \n",
    "    with jsonlines.open(output_file+'_'+lan+'.jsonl', mode='w') as writer:\n",
    "        writer.write_all(input_json)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ade9b-e674-403c-8961-9d7ca79be966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 done, batches left359\n",
      "batch 0 done, batches left359\n",
      "batch 1 done, batches left358\n",
      "batch 2 done, batches left357\n",
      "batch 1 done, batches left358\n",
      "batch 3 done, batches left356\n",
      "batch 4 done, batches left355\n",
      "batch 2 done, batches left357\n",
      "batch 5 done, batches left354\n",
      "batch 6 done, batches left353\n",
      "batch 3 done, batches left356\n",
      "batch 7 done, batches left352\n",
      "batch 8 done, batches left351\n",
      "batch 4 done, batches left355\n",
      "batch 9 done, batches left350\n",
      "batch 5 done, batches left354\n",
      "batch 10 done, batches left349\n",
      "batch 11 done, batches left348\n",
      "batch 6 done, batches left353\n",
      "batch 12 done, batches left347\n",
      "batch 13 done, batches left346\n",
      "batch 7 done, batches left352\n",
      "batch 14 done, batches left345\n",
      "batch 8 done, batches left351\n",
      "batch 15 done, batches left344\n",
      "batch 16 done, batches left343\n",
      "batch 9 done, batches left350\n",
      "batch 17 done, batches left342\n",
      "batch 18 done, batches left341\n",
      "batch 10 done, batches left349\n",
      "batch 19 done, batches left340\n",
      "batch 11 done, batches left348\n",
      "batch 20 done, batches left339\n",
      "batch 21 done, batches left338\n",
      "batch 12 done, batches left347\n",
      "batch 22 done, batches left337\n",
      "batch 23 done, batches left336\n",
      "batch 60 done, batches left299\n",
      "batch 109 done, batches left250\n",
      "batch 110 done, batches left249\n",
      "batch 61 done, batches left298\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "t1 = threading.Thread(target=translate, args=('input.jsonl','input_lang','ar'))\n",
    "t2 = threading.Thread(target=translate, args=('input.jsonl','input_lang','fr'))\n",
    "\n",
    "t1.start()\n",
    "t2.start()\n",
    "\n",
    "t1.join()\n",
    "t2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba40d8d-024a-41ce-9808-aae643f854bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "# get human line\n",
    "def getHumanSentences(input_file,output_file):\n",
    "    input_json = []\n",
    "    with open(input_file, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            if data['label'] == 'human':\n",
    "                input_json.append(data)\n",
    "    \n",
    "    with jsonlines.open(output_file, mode='w') as writer:\n",
    "        writer.write_all(input_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98f16a75-54a2-4f0d-b01c-29628d997063",
   "metadata": {},
   "outputs": [],
   "source": [
    "getHumanSentences('input_lang_zh-cn.jsonl','cn_human_lines.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b48767f-1070-4407-8c1a-0a2ff587f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 12000 of all lang\n",
    "import jsonlines\n",
    "def getsubset(num,input_file,output_file):\n",
    "    labels_count_map = {\n",
    "        'gpt2': 0,\n",
    "        'gptneo': 0,\n",
    "        'gptj': 0,\n",
    "        'llama': 0,\n",
    "        'gpt3re': 0,\n",
    "        'human': 0,\n",
    "    }\n",
    "    \n",
    "    input_json = []\n",
    "    with open(input_file, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            if labels_count_map[data['label']] < num:\n",
    "                labels_count_map[data['label']] += 1\n",
    "                input_json.append(data)\n",
    "\n",
    "            \n",
    "    with jsonlines.open(output_file, mode='w') as writer:\n",
    "        writer.write_all(input_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "604c6e27-266d-4648-816b-974ffe630ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "getsubset(1000,'input_lang_hi.jsonl','input_lang_hi_1000.jsonl')\n",
    "# getsubset(2000,'input_lang_fr.jsonl','input_lang_fr_small.jsonl')\n",
    "# getsubset(2000,'input_lang_ar.jsonl','input_lang_ar_small.jsonl')\n",
    "# getsubset(2000,'input_lang_zh-cn.jsonl','input_lang_zh-cn_small.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d6a860-951b-42de-ad1c-b007126ed716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_api(text, api_url, do_generate=False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param text: input text\n",
    "    :param api_url: api\n",
    "    :param do_generate: whether generate or not\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with httpx.Client(timeout=None) as client:\n",
    "        post_data = {\n",
    "            \"text\": text,\n",
    "            \"do_generate\": do_generate,\n",
    "        }\n",
    "        prediction = client.post(api_url,\n",
    "                                 data=msgpack.packb(post_data),\n",
    "                                 timeout=None)\n",
    "    if prediction.status_code == 200:\n",
    "        content = msgpack.unpackb(prediction.content)\n",
    "    else:\n",
    "        content = None\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb8e91b2-63db-4bc4-9d4a-aba235ea0193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(input_file, output_file, model_names, model_apis):\n",
    "    \"\"\"\n",
    "    get [losses, begin_idx_list, ll_tokens_list, label_int, label] based on raw lines\n",
    "    \"\"\"\n",
    "\n",
    "    en_labels = {\n",
    "        'gpt2': 0,\n",
    "        'gptneo': 1,\n",
    "        'gptj': 1,\n",
    "        'llama': 2,\n",
    "        'gpt3re': 3,\n",
    "        'gpt3sum': 3,\n",
    "        'human': 4,\n",
    "        'alpaca': None,\n",
    "        'dolly': None,\n",
    "    }\n",
    "\n",
    "\n",
    "    with open(input_file, 'r') as f:\n",
    "        lines = [json.loads(line) for line in f]\n",
    "\n",
    "    print('input file:{}, length:{}'.format(input_file, len(lines)))\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for data in tqdm(lines):\n",
    "            line = data['text']\n",
    "            # Split the text into sentences\n",
    "            sentences = re.split(r'[。！？]', line)\n",
    "\n",
    "            # Remove empty strings\n",
    "            sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "            \n",
    "            label = data['label']\n",
    "            \n",
    "            if label == 'human':\n",
    "                prompt_len = 0\n",
    "            else:\n",
    "                prompt_len = len(sentences[0])\n",
    "\n",
    "            losses = data['losses'] if 'losses' in data else []\n",
    "            begin_idx_list = data['begin_idx_list'] if 'begin_idx_list' in data else []\n",
    "            ll_tokens_list = data['ll_tokens_list'] if 'll_tokens_list' in data else []\n",
    "            \n",
    "            label_dict = en_labels\n",
    "\n",
    "            label_int = label_dict[label]\n",
    "\n",
    "            error_flag = False\n",
    "            for api in model_apis:\n",
    "                try:\n",
    "                    loss, begin_word_idx, ll_tokens = access_api(line, api)\n",
    "                except TypeError:\n",
    "                    print(\"return NoneType, probably gpu OOM, discard this sample\")\n",
    "                    error_flag = True\n",
    "                    break\n",
    "                losses.append(loss)\n",
    "                begin_idx_list.append(begin_word_idx)\n",
    "                ll_tokens_list.append(ll_tokens)\n",
    "            # if oom, discard this sample\n",
    "            if error_flag:\n",
    "                continue\n",
    "\n",
    "            result = {\n",
    "                'losses': losses,\n",
    "                'begin_idx_list': begin_idx_list,\n",
    "                'll_tokens_list': ll_tokens_list,\n",
    "                'label_int': label_int,\n",
    "                'label': label,\n",
    "                'text': line,\n",
    "                'prompt_len': prompt_len\n",
    "            }\n",
    "\n",
    "            f.write(json.dumps(result, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e34cb09-755b-4be6-bd37-c4131a44937d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input file:input_lang_fr.jsonl, length:35904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 18401/35904 [2:00:43<2:12:53,  2.20it/s]"
     ]
    }
   ],
   "source": [
    "get_features('input_lang_fr.jsonl','output_lang_fr.jsonl',['gpt_2','gpt2_french'],[gpt_2_api,gpt2_french])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e7d5f-8af5-4b82-a9d2-67e2557bf15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input file:input_lang_fr.jsonl, length:35904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 6830/35904 [38:20<2:36:28,  3.10it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 41%|████      | 14550/35904 [1:22:18<1:48:22,  3.28it/s]"
     ]
    }
   ],
   "source": [
    "get_features('input_lang_fr.jsonl','output_lang_fr_gptj.jsonl',['gptj_french'],[gptj_french])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b1fe1-a043-4f36-8805-22de6c6d1447",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_features('input_lang_fr.jsonl','output_lang_fr_llama.jsonl',['llama_api_french'],[llama_api_french])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16916d16-cd7f-44c1-b64c-bd27c8c3fb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35904\n"
     ]
    }
   ],
   "source": [
    " total = 0;\n",
    "with open('output_lang_fr_llama.jsonl', 'r') as file:\n",
    "    for line in file:\n",
    "        total = total + 1\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de873c9f-7695-489c-aaf4-6d18ebd8c740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    with jsonlines.open(file_path, 'r') as reader:\n",
    "        return list(reader)\n",
    "    \n",
    "def write_jsonl(file_path, data):\n",
    "    with jsonlines.open(file_path, 'w') as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "def write_jsonl(file_path, data):\n",
    "    with jsonlines.open(file_path, 'w') as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "\n",
    "input_jsonl = read_jsonl('output_prompt_fr_merged35K.jsonl')\n",
    "input_jsonl2 = read_jsonl('output_lang_fr_llama.jsonl')\n",
    "# input_jsonl3 = read_jsonl('output_lang_fr_llama.jsonl')\n",
    "\n",
    "i=0\n",
    "text = []\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "while i < len(input_jsonl):\n",
    "    if input_jsonl[i]['text'] == input_jsonl2[j]['text']:\n",
    "        input_jsonl[i]['losses'].extend(input_jsonl2[j]['losses'])\n",
    "        input_jsonl[i]['begin_idx_list'].extend(input_jsonl2[j]['begin_idx_list'])\n",
    "        input_jsonl[i]['ll_tokens_list'].extend(input_jsonl2[j]['ll_tokens_list'])\n",
    "        i = i + 1\n",
    "        j = j + 1\n",
    "    else:\n",
    "        text.append(input_jsonl2[j])\n",
    "        j = j+1\n",
    "\n",
    "\n",
    "print(len(text))\n",
    "write_jsonl('output_prompt_fr_merged35K.jsonl',input_jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d14445b3-ffcb-4edd-8b1f-b130f2c31f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    with jsonlines.open(file_path, 'r') as reader:\n",
    "        return list(reader)\n",
    "    \n",
    "def write_jsonl(file_path, data):\n",
    "    with jsonlines.open(file_path, 'w') as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "def write_jsonl(file_path, data):\n",
    "    with jsonlines.open(file_path, 'w') as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "\n",
    "input_jsonl = read_jsonl('output_lang_zh-cn_new.jsonl')\n",
    "\n",
    "i=0\n",
    "for item in input_jsonl:\n",
    "    if len(item['losses']) < 2:\n",
    "        i=i+1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11161b72-1340-4abc-9e5b-1f4569a976f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    with jsonlines.open(file_path, 'r') as reader:\n",
    "        return list(reader)\n",
    "    \n",
    "def write_jsonl(file_path, data):\n",
    "    with jsonlines.open(file_path, 'w') as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "def write_jsonl(file_path, data):\n",
    "    with jsonlines.open(file_path, 'w') as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "\n",
    "input_jsonl = read_jsonl('output_prompt_zh-cn_merged35K.jsonl')\n",
    "input_jsonl2 = read_jsonl('output_lang_zh-cn_llama.jsonl')\n",
    "\n",
    "\n",
    "i=0\n",
    "for item1 in input_jsonl:\n",
    "    if i==0:\n",
    "        print(len(item1['begin_idx_list']))\n",
    "    i=i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4e114c1-35ee-4e94-88aa-b83e16b98fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pmukher/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    with jsonlines.open(file_path, 'r') as reader:\n",
    "        return list(reader)\n",
    "    \n",
    "def write_jsonl(file_path, data):\n",
    "    with jsonlines.open(file_path, 'w') as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "def write_jsonl(file_path, data):\n",
    "    with jsonlines.open(file_path, 'w') as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "\n",
    "input_jsonl = read_jsonl('output_prompt_fr_merged35K.jsonl')\n",
    "\n",
    "for item1 in input_jsonl:\n",
    "    if item1['label'] != 'human' :\n",
    "        sent_separator = nltk.data.load('tokenizers/punkt/french.pickle')\n",
    "        sents = sent_separator.tokenize(item1['text'])\n",
    "        item1['prompt_len'] = len(sents[0])\n",
    "\n",
    "\n",
    "write_jsonl('output_prompt_fr_merged35K.jsonl',input_jsonl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "341da0e7-8a72-4913-9eb9-dae040a5580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    with jsonlines.open(file_path, 'r') as reader:\n",
    "        return list(reader)\n",
    "    \n",
    "def write_jsonl(file_path, data):\n",
    "    with jsonlines.open(file_path, 'w') as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "def write_jsonl(file_path, data):\n",
    "    with jsonlines.open(file_path, 'w') as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "\n",
    "input_jsonl = read_jsonl('output_all_zh-cn_small.jsonl')\n",
    "output_jsonl = read_jsonl('output_all_zh-cn_final.jsonl')\n",
    "\n",
    "i = 0\n",
    "for item1 in input_jsonl:\n",
    "    if output_jsonl[i]['text'] == item1['text']:\n",
    "        if 'prompt_len' in item1 :\n",
    "            output_jsonl[i]['prompt_len'] = item1['prompt_len']\n",
    "        else:\n",
    "            output_jsonl[i]['prompt_len'] = 0\n",
    "        i = i + 1\n",
    "\n",
    "write_jsonl('output_prompt_zh-cn_merged.jsonl',output_jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "207c5e16-912c-4a56-896d-12b4ba7efa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data binary\n",
    "import jsonlines\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    with jsonlines.open(file_path, 'r') as reader:\n",
    "        return list(reader)\n",
    "    \n",
    "def write_jsonl(file_path, data):\n",
    "    with jsonlines.open(file_path, 'w') as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "binary_data = read_jsonl('output_prompt_zh-cn_merged35K.jsonl')\n",
    "\n",
    "i=0\n",
    "labels = []\n",
    "for item in binary_data:\n",
    "    if len(item['losses'])==2:\n",
    "        i = i+1\n",
    "        labels.append(item['label'])\n",
    "        \n",
    "element_counts = Counter(labels)\n",
    "\n",
    "# Print the counts\n",
    "for element, count in element_counts.items():\n",
    "    print(f\"{element}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2853fb7-f700-4061-a3e3-461c8133a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data binary\n",
    "import jsonlines\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    with jsonlines.open(file_path, 'r') as reader:\n",
    "        return list(reader)\n",
    "    \n",
    "def write_jsonl(file_path, data):\n",
    "    with jsonlines.open(file_path, 'w') as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "binary_data = read_jsonl('output_prompt_fr_merged35K.jsonl')\n",
    "\n",
    "for item in binary_data:\n",
    "    if item['label'] != 'human':\n",
    "        item['label'] = 'AI'\n",
    "\n",
    "write_jsonl('output_binary_merged_fr.jsonl',binary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4023a36-e545-4a41-87f6-d400f553a857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "zh-cn\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def detect_language(sentence):\n",
    "    return detect(sentence)\n",
    "\n",
    "# Example usage\n",
    "english_sentence = \"This is an English sentence.\"\n",
    "chinese_sentence = \"今年三月，政府宣布了任命一名东安格利亚市长的计划，但遭到多个议会的拒绝。据BBC了解，现在地方议会和政府的领导人已经提出了新的建议。但该计划仍需获得剑桥郡、诺福克郡和萨福克郡所有 23 个议会的批准。该计划还需要得到剑桥郡的批准\\n无能领导者的危险\\n我相信所有商界领袖都应该至少每周思考并做出以下承诺，因为这不仅对公司的健康至关重要，而且对经济乃至整个国家都至关重要。此外，需要注意的是，每位领导者必须代表每位团队成员做出承诺，反之亦然。换句话说，如果组织中的所有 200 人都不参与，则领导者也不参与。我坚信，我们作为一个经济体的成功、我们的生活质量，甚至我们作为一个国家的安全和保障，都在很大程度上取决于我们每个人尽最大努力确保我们组织中的每个团队成员都在做对我们国家来说可能是最成功的工作。\"\n",
    "\n",
    "print(detect_language(english_sentence))  # Output: en (English)\n",
    "print(detect_language(chinese_sentence)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bf74c75-5f3d-44fa-b6d1-64e940a4a834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langdetect\n",
      "  Using cached langdetect-1.0.9.tar.gz (981 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=22a6b10ea42be676da991bd34db262400663aacf474919d7f586d5bbc1368bfe\n",
      "  Stored in directory: /home/pmukher/.cache/pip/wheels/d1/c1/d9/7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "793f7c2e-021c-44c3-9e13-3422a3c71d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import re\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    with jsonlines.open(file_path, 'r') as reader:\n",
    "        return list(reader)\n",
    "    \n",
    "def write_jsonl(file_path, data):\n",
    "    with jsonlines.open(file_path, 'w') as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "binary_data = read_jsonl('output_binary_merged_zh-cn.jsonl')\n",
    "\n",
    "for item in binary_data:\n",
    "    line = item['text']\n",
    "    # Split the text into sentences\n",
    "    sentences = re.split(r'[。！？]', line)\n",
    "\n",
    "    item['prompt_len'] = len(sentences[0])\n",
    "        \n",
    "write_jsonl('output_binary_merged_zh-cn.jsonl',binary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e365671-6143-4bcc-8091-3e1b0d673b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "def detect_language(sentence):\n",
    "    return detect(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03b15c3d-2465-4656-9744-1f46625facde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import re\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    with jsonlines.open(file_path, 'r') as reader:\n",
    "        return list(reader)\n",
    "    \n",
    "def write_jsonl(file_path, data):\n",
    "    with jsonlines.open(file_path, 'w') as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "binary_data = read_jsonl('output_binary_merged_zh-cn.jsonl')\n",
    "\n",
    "with open('output_binary_merged_zh-cn_new.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in binary_data:\n",
    "        if detect_language(item['text']) == 'en':\n",
    "            i = i + 1\n",
    "        else:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46ceff4f-dc09-479c-ac43-4c479c9e0438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35743\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import re\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    with jsonlines.open(file_path, 'r') as reader:\n",
    "        return list(reader)\n",
    "    \n",
    "def write_jsonl(file_path, data):\n",
    "    with jsonlines.open(file_path, 'w') as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "binary_data = read_jsonl('output_binary_merged_zh-cn_new.jsonl')\n",
    "\n",
    "print(len(binary_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fa7e8d-0f80-4806-9e3d-ee32a7fd86f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ORC)",
   "language": "python",
   "name": "sys_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
